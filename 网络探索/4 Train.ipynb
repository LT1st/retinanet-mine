{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.0 Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9d6977def170>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "\n",
    "from retinanet import model\n",
    "from retinanet.dataloader import CocoDataset, CSVDataset, collater, Resizer, AspectRatioBasedSampler, Augmenter, \\\n",
    "    Normalizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from retinanet import coco_eval\n",
    "from retinanet import csv_eval\n",
    "\n",
    "assert torch.__version__.split('.')[0] == '1'\n",
    "\n",
    "print('CUDA available: {}'.format(torch.cuda.is_available()))\n",
    "\n",
    "\n",
    "def main(args=None):\n",
    "    parser = argparse.ArgumentParser(description='Simple training script for training a RetinaNet network.')\n",
    "\n",
    "    parser.add_argument('--dataset', default='coco', help='Dataset type, must be one of csv or coco.')  # 设定所采用数据集的名字为‘coco’\n",
    "    parser.add_argument('--coco_path', default='coco', help='Path to COCO directory')  # 设定coco数据集所在的路径\n",
    "    parser.add_argument('--csv_train', help='Path to file containing training annotations (see readme)')  # 设定自己的数据集中train部分图片的路径\n",
    "    parser.add_argument('--csv_classes', help='Path to file containing class list (see readme)')  # 设定自己的数据集中图片类别信息的路径\n",
    "    parser.add_argument('--csv_val', help='Path to file containing validation annotations (optional, see readme)')  # 设定自己的数据集中val部分图片的路径\n",
    "\n",
    "    parser.add_argument('--depth', default=50, help='Resnet depth, must be one of 18, 34, 50, 101, 152', type=int)  # 设定所采用resnet的深度\n",
    "    parser.add_argument('--epochs', help='Number of epochs', type=int, default=100)  # 设定模型所跑的epoch的总数\n",
    "\n",
    "    parser = parser.parse_args(args)\n",
    "\n",
    "    # Create the data loaders\n",
    "    if parser.dataset == 'coco':\n",
    "\n",
    "        if parser.coco_path is None:\n",
    "            raise ValueError('Must provide --coco_path when training on COCO,')\n",
    "\n",
    "        # 读入训练的数据，并通过Normalizer(), Augmenter(), Resizer()三个工具对数据进行预处理\n",
    "        dataset_train = CocoDataset(parser.coco_path, set_name='train2017',\n",
    "                                    transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "        # 读入验证的数据，相较于train的数据少了Augmenter()的预处理部分\n",
    "        dataset_val = CocoDataset(parser.coco_path, set_name='val2017',\n",
    "                                  transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "    elif parser.dataset == 'csv':\n",
    "\n",
    "        if parser.csv_train is None:\n",
    "            raise ValueError('Must provide --csv_train when training on COCO,')\n",
    "\n",
    "        if parser.csv_classes is None:\n",
    "            raise ValueError('Must provide --csv_classes when training on COCO,')\n",
    "\n",
    "        dataset_train = CSVDataset(train_file=parser.csv_train, class_list=parser.csv_classes,\n",
    "                                   transform=transforms.Compose([Normalizer(), Augmenter(), Resizer()]))\n",
    "\n",
    "        if parser.csv_val is None:\n",
    "            dataset_val = None\n",
    "            print('No validation annotations provided.')\n",
    "        else:\n",
    "            dataset_val = CSVDataset(train_file=parser.csv_val, class_list=parser.csv_classes,\n",
    "                                     transform=transforms.Compose([Normalizer(), Resizer()]))\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Dataset type not understood (must be csv or coco), exiting.')\n",
    "\n",
    "    # AspectRatioBasedSampler函数的作用为将dataset_train的数据变成以batch_size一个一个group\n",
    "    sampler = AspectRatioBasedSampler(dataset_train, batch_size=1, drop_last=False)\n",
    "    # collater返回图片[batch_size,h,w,c]，标记[batchsize,?,5]（其中？由有最多标记数目的图片决定，无用的标记以[-1,-1,-1,-1,-1]表示）\n",
    "    dataloader_train = DataLoader(dataset_train, num_workers=3, collate_fn=collater, batch_sampler=sampler)\n",
    "\n",
    "    if dataset_val is not None:\n",
    "        sampler_val = AspectRatioBasedSampler(dataset_val, batch_size=1, drop_last=False)\n",
    "        dataloader_val = DataLoader(dataset_val, num_workers=3, collate_fn=collater, batch_sampler=sampler_val)\n",
    "\n",
    "    # Create the model 读入模型，且需要有预训练的数据 \n",
    "    if parser.depth == 18:\n",
    "        retinanet = model.resnet18(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif parser.depth == 34:\n",
    "        retinanet = model.resnet34(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif parser.depth == 50:\n",
    "        retinanet = model.resnet50(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif parser.depth == 101:\n",
    "        retinanet = model.resnet101(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    elif parser.depth == 152:\n",
    "        retinanet = model.resnet152(num_classes=dataset_train.num_classes(), pretrained=True)\n",
    "    else:\n",
    "        raise ValueError('Unsupported model depth, must be one of 18, 34, 50, 101, 152')\n",
    "\n",
    "    use_gpu = True\n",
    "\n",
    "    if use_gpu:  # 判断是否有GPU，有则用\n",
    "        if torch.cuda.is_available():\n",
    "            retinanet = retinanet.cuda()\n",
    "\n",
    "    if torch.cuda.is_available():  # 用多个GPU\n",
    "        retinanet = torch.nn.DataParallel(retinanet).cuda()\n",
    "    else:\n",
    "        retinanet = torch.nn.DataParallel(retinanet)\n",
    "\n",
    "    # 将retinanet设定为训练状态\n",
    "    retinanet.training = True\n",
    "\n",
    "    # 优化器为Adam,learning_rate为0.5（实际论文中为SGD）\n",
    "    optimizer = optim.Adam(retinanet.parameters(), lr=1e-5)\n",
    "\n",
    "    # class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10,\n",
    "    #                                                  verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)\n",
    "    # optimer指的是网络的优化器\n",
    "    # mode (str) ，可选择‘min’或者‘max’，min表示当监控量停止下降的时候，学习率将减小，max表示当监控量停止上升的时候，学习率将减小。默认值为‘min’\n",
    "    # factor 学习率每次降低多少，new_lr = old_lr * factor\n",
    "    # patience=10，容忍网路的性能不提升的次数，高于这个次数就降低学习率\n",
    "    # verbose（bool） - 如果为True，则为每次更新向stdout输出一条消息。 默认值：False\n",
    "    # threshold（float） - 测量新最佳值的阈值，仅关注重大变化。 默认值：1e-4\n",
    "    # cooldown： 减少lr后恢复正常操作之前要等待的时期数。 默认值：0。\n",
    "    # min_lr,学习率的下限\n",
    "    # eps ，适用于lr的最小衰减。 如果新旧lr之间的差异小于eps，则忽略更新。 默认值：1e-8。\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, verbose=True)\n",
    "\n",
    "    loss_hist = collections.deque(maxlen=500)  # 用collection存loss，最多500个\n",
    "\n",
    "    retinanet.train()\n",
    "    retinanet.module.freeze_bn()  # 固定bn不变\n",
    "\n",
    "    print('Num training images: {}'.format(len(dataset_train)))\n",
    "\n",
    "    for epoch_num in range(parser.epochs):\n",
    "\n",
    "        retinanet.train()\n",
    "        retinanet.module.freeze_bn()\n",
    "\n",
    "        epoch_loss = []\n",
    "\n",
    "        for iter_num, data in enumerate(dataloader_train):\n",
    "            try:\n",
    "                optimizer.zero_grad()  # 将上一步中的Gradient置0\n",
    "\n",
    "                if torch.cuda.is_available():  # 将图片和annotation输入模型，获得两个loss\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].cuda().float(), data['annot']])\n",
    "                else:\n",
    "                    classification_loss, regression_loss = retinanet([data['img'].float(), data['annot']])\n",
    "\n",
    "                # 将两个loss做平均\n",
    "                classification_loss = classification_loss.mean()\n",
    "                regression_loss = regression_loss.mean()\n",
    "\n",
    "                # 求和\n",
    "                loss = classification_loss + regression_loss\n",
    "\n",
    "                if bool(loss == 0):\n",
    "                    continue\n",
    "\n",
    "                # 求导\n",
    "                loss.backward()\n",
    "\n",
    "                # 实现了Clipping Gradient，避免梯度爆炸的出现\n",
    "                torch.nn.utils.clip_grad_norm_(retinanet.parameters(), 0.1)\n",
    "\n",
    "                # 应用gradient到模型的变量上去\n",
    "                optimizer.step()\n",
    "\n",
    "                # 记录loss\n",
    "                loss_hist.append(float(loss))\n",
    "\n",
    "                epoch_loss.append(float(loss))\n",
    "\n",
    "                print(\n",
    "                    'Epoch: {} | Iteration: {} | Classification loss: {:1.5f} | Regression loss: {:1.5f} | Running loss: {:1.5f}'.format(\n",
    "                        epoch_num, iter_num, float(classification_loss), float(regression_loss), np.mean(loss_hist)))\n",
    "\n",
    "                del classification_loss\n",
    "                del regression_loss\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "        if parser.dataset == 'coco':\n",
    "\n",
    "            print('Evaluating dataset')\n",
    "\n",
    "            # 用val集来评价模型性能\n",
    "            coco_eval.evaluate_coco(dataset_val, retinanet)\n",
    "\n",
    "        elif parser.dataset == 'csv' and parser.csv_val is not None:\n",
    "\n",
    "            print('Evaluating dataset')\n",
    "\n",
    "            mAP = csv_eval.evaluate(dataset_val, retinanet)\n",
    "\n",
    "        # 更新优化器的参数\n",
    "        scheduler.step(np.mean(epoch_loss))\n",
    "\n",
    "        # 存下模型\n",
    "        torch.save(retinanet.module, '{}_retinanet_{}.pt'.format(parser.dataset, epoch_num))\n",
    "\n",
    "    retinanet.eval()\n",
    "\n",
    "    torch.save(retinanet, 'model_final.pt')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
